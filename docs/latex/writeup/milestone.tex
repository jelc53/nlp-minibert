\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{arydshln}
\usepackage[left=1.5cm, right=1.5cm]{geometry}


\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Walk Less and Only Down Smooth Valleys \\
  \vspace{0.15cm}
  \small{\normalfont Stanford CS224N Default Project Milestone}  % Select one and delete the other
}

\author{
  Quinn Hollister \\
  % ICME \\
  Stanford University \\
  \texttt{bh9vw@stanford.edu} \\
  % Examples of more authors
   \And
    Julian Cooper \\
  % ICME \\
  Stanford University \\
  \texttt{jelc@stanford.edu} \\
   \And
   Thomas Brink \\
  % ICME \\
  Stanford University \\
  \texttt{tbrink@stanford.edu} \\
}

\begin{document}

\maketitle
\vspace{-0.4cm}
\begin{abstract}
  Transfer learning has become a valuable tool for handling a variety of downstream NLP tasks given a single generic pre-trained model \cite{weiss2016survey}. Since most of the downstream tasks are specialized, quality training data is a scarce resource, making training large models from scratch very difficult. 
  In this project, we investigate how the performance of a pre-trained BERT encoder model changes for three different downstream prediction tasks when we include (1) additional pre-training on target-domain data, (2) additional fine-tuning on more representative mix of downstream tasks, and (3) regularization in our fine-tuning loss function and parameter (SMART by Jiang et al. \cite{smart}). 
  We find that SMART prevents overfitting that is common in fine-tuning BERT models to downstream tasks. This adversarial regularization is especially effective when combined with multi-task learning. 
\end{abstract}
\vspace{-0.5cm}
% {\color{red} This template does not contain the full instruction set for this assignment; please refer back to the milestone instructions PDF.}

\section{Approach}
% \item Please be specific when describing your main approaches. You may want to include key equations and figures (though it is fine if you want to defer creating time-consuming figures until the final report).

\textbf{Model extensions}. In addition to the default minBERT model, this project entails three extensions:
\begin{enumerate}
    \item \textbf{Additional pre-training for target domain} \cite{pretrain}: While we expect our pre-trained minBERT model weights to be effective for paraphrasing and semantic similarity analyses, we assume it will struggle to classify sentiment. This is because the Wikipedia corpus is largely filled with non-emotive, informational language. Therefore, we want to add an incremental pre-training layer that includes more emotive language (e.g. MULTIOpEd \cite{multioped}) trained on masked language modeling and next sentence prediction (original BERT objectives \cite{bert}). 

    \item \textbf{Round-robin multitask fine-tuning} \cite{handout}: The default implementation assumes that fine-tuning only on sentiment classification for SST dataset will generalize well to paraphrasing and similarity prediction tasks. Even superficial experiments proved to us that this was not true! To address this, we have implemented a batch-level round-robin routine. For each batch iteration, we cycle through training dataloaders for SST, Quora and STS and perform one update for each so our fine-tuned model will see examples for all tasks. The updates use cross entropy, binary cross entropy and cosine similarity loss respectively.

    \item \textbf{Regularization of fine-tuning loss and optimizer step} \cite{smart}: Many fine-tuning routines suffer from overfitting, which leads to poor performance on test sets of downstream prediction tasks. Having carefully included corpus text from relevant domains in our pre-training steps, we do not want our fine-tuning to diverge too rapidly from the pre-trained weights. For this, we make use of regularization techniques by implementing SMART (see below for details).
\end{enumerate}

For this milestone, we have implemented minBERT with AdamW, as well as a version of (2) and (3) from the above extensions. The results are shown in the experiments section. Given the details of SMART regularization are the most complex, we include a section summarizing its math and logic.

% \item If any part of your approach is original, make it clear. For models and techniques that are not yours, provide references.

\textbf{SMART implementation.} To effectively control the \textbf{extremely high complexity} of the LLM, Jiang et al. \cite{smart} propose a smoothness-inducing adversarial regularization technique. The desired property is that when the input $x$ is perturbed by a small amount, the output should not change much. To achieve this, Jiang et al. \cite{smart} optimize loss $\mathcal{F}(\theta)$ using: $\min_{\theta} \mathcal{F}(\theta) = \mathcal{L}(\theta) + \lambda_{s}\mathcal{R}_{s}(\theta)$ where,
\begin{align*}
\mathcal{L}(\theta) & = \frac{1}{n} \sum_{i=1}^{n} l(f(x_{i};\theta), y_{i}) && \text{regular loss function} \\
\mathcal{R}_{s}(\theta) & = \frac{1}{n} \sum_{i=1}^{n} \max_{\lVert \Tilde{x_{i}} - x_{i} \rVert_{p \leq \epsilon}} l_{s}\left(f(\Tilde{x_{i}}; \theta), f(x_{i}; \theta)\right) && \text{regularization term}\\
l_{s}(P, Q) & = \mathcal{D}_{KL}(P \lVert Q) + \mathcal{D}_{KL}(Q \lVert P) && \text{symmetric KL-divergence}
\end{align*} 

% Here, $\mathcal{L}(\theta)$ is a `regular' loss function on model $f$, and $\lambda_s \mathcal{R}_s(\theta)$ is a regularization term, the computation of which requires a maximization problem that can be solved efficiently using projected gradient ascent (3). 
Note that this regularizer term is measuring the local Lipschitz continuity under the symmetrized KL-divergence metric. So, the output of our model does not change much if we inject a small perturbation (constrained to be $\epsilon$ small in the $p$-euclidean metric) to the input. Thus, we can encourage our model $f$ to be smooth within the neighborhoods of our inputs. This is particularly helpful when working in a low-resource domain task. 

To prevent \textbf{aggressive updating}, the optimization routine is changed so that we introduce a trust-region-type regularization at each iteration, so we only update the model within a small neighborhood of the previous iterate. In order to solve the regularizer term, Jiang et al. \cite{smart} develop a class of Bregman proximal point optimization methods. Specifically, we update parameters $\theta$ by: $\theta_{t+1} = \text{argmin}_{\theta} \mathcal{F}(\theta) + \mu \mathcal{D}_{Breg}(\theta, \Tilde{\theta_{t}})$, where,
\begin{align*}
\mathcal{D}_{Breg}(\theta, \Tilde{\theta_{t}}) & = \frac{1}{n} \sum_{i=1}^{n} l_{s} \left( f(x_{i}; \theta), f(x_{i}; \Tilde{\theta_{t}}) \right) && \text{Bregman divergence} \\
\Tilde{\theta}_{t} & = (1 - \beta)\theta_{t} + \beta \Tilde{\theta}_{t-1} && \text{regularized update step}
\end{align*}

% \item Describe your baselines. Depending on space constraints and how standard your baseline is, you might do this in detail or simply refer to other papers for details. Default project teams can do the latter when describing the provided baseline model. Our baseline for the default project is outlined in \cite{bert}.

\textbf{Baselines}. First, we compare our sentiment analysis accuracy with the baseline scores presented in the default project handout. These scores include baseline accuracies for both pre-trained and finetuned models on the SST and CFIMDB datasets. We also verify that multitask classification for default pretrain and finetune matches our single-task classifier results for sentiment analysis of SST dataset. Mostly, we can view the handout benchmarks as `correctness' tests for our implementation. Second, we compare all of our models (i.e. pretrain default, finetune default, finetune with extensions) against each other across the three prediction tasks evaluated by the multitask classifier. Here, we treat the finetune default BERT model defined in section 3 of the handout as our baseline. Since the idea behind the regularization used for our extensions is to prevent overfitting during fine-tuning, we would hope our final model improves on our benchmarks. 

% \item If you are using any code that you did not write yourself, make it clear and provide a reference or link. When describing something you coded yourself, make it clear.


\section{Experiments}
%In this section we discuss datasets, evaluation method, experiments performed and results so far.

% \textbf{Data}: Describe the dataset(s) you are using along with references. Make sure the task associated with the dataset is clearly described.
\textbf{Data}. For fine-tuning and evaluating our model on downstream prediction tasks, we use the Stanford Sentiment Treebank, Quora Dataset, and SemEval STS Benchmark Dataset. The \emph{Stanford Sentiment Treebank} (SST) dataset consists of 11,855 single sentence reviews, where a review is labelled categorically \{negative, somewhat negative, neutral, somewhat positive, positive\}. The \emph{Quora Dataset} (Quora) consists of 400,000 question pairs with binary labels (true if one question is paraphrasing the other). And, finally, the \emph{SemEval STS Benchmark Dataset} (STS) consists of 8,628 different sentence pairs, with each given a score from 0 (unrelated) to 5 (equivalent meaning). 
% Our datasets require some minimal pre-processing, including tokenizing sentence strings, lower-casing word tokens, standardizing punctuation tokens, and padding sentences to enable matrix multiplication.


% \item \textbf{Evaluation method}: Describe the evaluation metric(s) you used, plus any other details necessary to understand your evaluation.
\textbf{Evaluation method}. We use accuracy for the sentiment analysis and paraphrase detection tasks, and Pearson correlation for semantic textual similarity. We note that the sentiment analysis accuracy is based on multi-class classification, while the paraphrase detection task is binary. 

% \item \textbf{Experimental details}: Please explain how you ran your experiments (e.g. model configurations, learning rate, training time, etc.).
\textbf{Experimental details}. Beyond our baseline results (default pretrain and finetune), we ran experiments for each model extension we have implemented so far (extensions 2 and 3). In addition to having default settings, i.e., 10 training epochs, pretrain learning rate of 1e-3 and finetune learning rate of 1e-5, the following describes configuration specifics:

\begin{itemize}
    \item \textbf{Ext 2: Round-robin fine-tuning} (rrobin): we set the number of iterations (over batches from all three datasets) to be \texttt{floor(len(sts\_train\_data) / args.batch\_size)}, where STS represents the smallest task-specific training dataset. In this way, we guarantee training equally across all three tasks, which helps balance our finetuning process (otherwise Quora may dominate), but comes at the cost of throwing away potentially informative training data.
    
    \item \textbf{Ext 3: SMART regularization} (smart): The hyperparameter values specific to the SMART \cite{smart} implementation were $\lambda = 1$, $\epsilon = 1e-5$, $\sigma = 1e-6$, $\beta = 0.995$, $\mu = 1$, $\eta = 1e-3$ and K = 1. Due to memory usage, we reduced our batch size to 8 rather than 64. 
\end{itemize}

% \item \textbf{Results}: Report the quantitative results that you have so far. Use a table or plot to compare multiple results and compare against your baselines.

\textbf{Results}
Table \ref{tab: single} compares the dev accuracy of our default model (pretrain and finetune) with benchmarks provided in the handout for the single-task classifier. Our results are all close to the, benchmarks which gives us confidence in the `correctness' of our default implementation. 
\begin{table}[h]
\footnotesize
\centering
\begin{tabular}{|l|ccc|ccc|}
\hline
  & \multicolumn{3}{c|}{\textbf{Sentiment (SST)}} & \multicolumn{3}{c|}{\textbf{Sentiment (CFIMDB)}} \\ \hline
\textbf{Model type}       & Accuracy       & Benchmark & Runtime (s)          & Accuracy        & Benchmark & Runtime (s)           \\ \hline
Pretrain default & 0.393           & 0.390 (0.007) & 30    & 0.788            & 0.780 (0.002)  & 45     \\
Finetune default & 0.522           & 0.515 (0.004) & 120    & 0.963            & 0.966 (0.007) & 150      \\ \hline
\end{tabular}
\caption{Dev accuracy of default model vs. benchmarks for single-task classifier and runtimes per training epoch on a Google Colab GPU.}
\label{tab: single}
\end{table}
\vspace{-0.5cm}

Table \ref{tab: multi} compares dev accuracies of our default implementation (treated as our baseline from now on) with the different model extensions. As hoped, both our batch-level round-robin and SMART regularization extensions independently improved our model's overall performance. In particular, we see improvements in the paraphrase and similarity tasks. Our best result comes from combining both extensions (rrobin+smart), although this does significantly increase computation time. 
\begin{table}[h]
\footnotesize
\centering
\begin{tabular}{|l|cccc|} \hline
& \multicolumn{1}{c}{\textbf{Sentiment (SST)}} & \multicolumn{1}{c}{\textbf{Paraphrase (Quora)}} & \multicolumn{1}{c}{\textbf{Similarity (STS)}} &  \\ \hline
\textbf{Model type} & Accuracy & Accuracy & Correlation & Runtime (s) \\ \hline
Pretrain default      & 0.396 (*)         & 0.380            & 0.019 & 9          \\
Finetune default      & 0.525 (*)        & 0.522           & 0.240 &  25         \\
Finetune rrobin       & 0.524            & 0.726            & 0.583   & 67        \\
Finetune smart        & 0.520             & 0.501           & 0.382 & 161          \\
Finetune rrobin+smart & 0.532              & 0.741            & 0.680 & 464          \\ \hline
\end{tabular}
\caption{Dev set accuracies of default vs. model extensions for multi-task classifier and runtimes per training epoch on AWS EC2 instance.}
\label{tab: multi}
\end{table}
\vspace{-0.6cm}
% (*) As a sense check, we verify that default pretrain and finetune accuracies approximately match results from single-task classifier.

% \begin{center}
% \begin{tabular}{||c |c : c|c : c|c : c||}
%     \hline
%         Model & \multicolumn{2}{ c |}{Sentiment (accuracy)} & \multicolumn{2}{ c |}{Paraphrase (accuracy)} & \multicolumn{2}{ c ||}{Similarity (correlation)} \\
%     \hline \hline 
%         pre-baseline & .78 & .78 & .78 & .78  &  .78    & .78 \\ 
%     \hline
%         finetune-baseline  & .78  & .78  & .78  & .78  &  .78 &  .78  \\
%     \hline
%         SMART & .78  &  .78  &  .78 &  .78  &  .78   &  .78 \\
%     \hline
% \end{tabular}
% \end{center}

\section{Future work}
    In their development of the SMART framework, Jiang et al. found that combining SMART with multi-task learning achieved the best benchmark performance results \cite{smart}. In a similar vein, we would like to explore how MTL could improve our model and combine it with gradient surgery in order to optimize our model on  all of the tasks instead of just the semantic similarity task. Also, our model has a number of hyperparameters and trying to find the optimal combination of these is computationally expensive and time-consuming. One idea we are eager to try is extending the bayesian optimization framework described by Nando de Freitas et. al. \cite{BayesianOpt}. In addition, as discussed, we plan to incorporate additional pre-training as our third extension.

\newpage
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}